# fmw_dm_single_ux3.py
# ë‹¨ì¼ íŒŒì¼: Streamlit ë°ì´í„° ë§¤ë‹ˆì € (UX ë¦¬ë‰´ì–¼ v3, ëª¨ë°”ì¼ ì¹œí™”)
# - PRD v1.1 & ERD ë°˜ì˜: GET-only API(v1), 06:00 ìë™ ë™ê¸°í™”, ì˜¤ë²„ë·° 4ì¹´ë“œ(+7ì¼ ì¶”ì´), íƒìƒ‰, ë³€ê²½ ì´ë ¥
# - APScheduler ì—†ì„ ë•Œ Fallback: "ì•± ì—´ë¦´ ë•Œ í•˜ë£¨ 1íšŒ" ìë™ ë™ê¸°í™”
# - ìƒ˜í”Œ ë°ì´í„°(ë‚´ì¥) í¬í•¨: 10ê°œ ëª¨ë¸ Ã— 4ê°œ ê·¸ë£¹(allow list / block list / rel features / ue capa)
# - ì°¨ì›(dims): mcc, mnc, region, country, operator, sp_type, model_name, mode
# - ë‹¤ìš´ë¡œë“œ: CSV(+ê°€ëŠ¥í•˜ë©´ Excel)
#
# ì‹¤í–‰ ì˜ˆì‹œ
#   pip install streamlit pandas requests pytz python-dotenv   # apscheduler/xlsxwriterëŠ” ì„ íƒ
#   USE_SAMPLE=1 streamlit run fmw_dm_single_ux3.py
#   USE_SAMPLE=0 API_BASE="http://localhost:8000/api" API_KEY="..." streamlit run fmw_dm_single_ux3.py

import os, io, json, pathlib, base64
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List

import pandas as pd
import requests
import streamlit as st
from pytz import timezone as tz

# ============== ì„ íƒ ì˜ì¡´ì„±: APScheduler / xlsxwriter ==============
try:
    from apscheduler.schedulers.background import BackgroundScheduler  # type: ignore
except Exception:
    BackgroundScheduler = None  # íŒ¨í‚¤ì§€ ì—†ìœ¼ë©´ Fallback ì‚¬ìš©

try:
    import xlsxwriter  # noqa: F401  # ì¡´ì¬ ì—¬ë¶€ë§Œ í™•ì¸
    XLSX_AVAILABLE = True
except Exception:
    XLSX_AVAILABLE = False

# ============== ì„¤ì • ==============
KST = tz("Asia/Seoul")
DATA_DIR = pathlib.Path("./_cache_v1")
DATA_DIR.mkdir(parents=True, exist_ok=True)

USE_SAMPLE = os.getenv("USE_SAMPLE", "1")  # ê¸°ë³¸ ìƒ˜í”Œ ON
API_BASE = os.getenv("API_BASE", "http://localhost:8000/api").rstrip("/")
API_KEY  = os.getenv("API_KEY")
VERIFY_SSL = os.getenv("VERIFY_SSL", "false").lower() in ("1", "true", "yes")

# API ê²½ë¡œ (PRD v1.1 ë°˜ì˜: GET ì „ìš©)
PATH_GROUPS   = "/feature-groups/"
PATH_FEATURES = "/features/"
PATH_RECORDS  = "/feature-records/"
PATH_LONG     = "/long-records/"  # ì„ íƒ

# ============== ìƒ˜í”Œ ë°ì´í„°(ë‚´ì¥ CSV) ==============
# PRD: dims = [region,country,operator,sp_type,mcc,mnc]; í•„ë“œ í†µì¼: sp_type ì‚¬ìš©
SAMPLE_CSV = """feature_group,feature_name,model_name,mcc,mnc,region,country,operator,sp_type,mode,value,status,updated_at
allow list,device_allowed,M100,450,5,APAC,KR,KT,SP-001,allow,true,active,2025-10-23T06:00:00+09:00
block list,blocked_reason,M100,450,5,APAC,KR,KT,SP-001,block,,active,2025-10-23T06:00:00+09:00
rel features,nr_release,M100,450,5,APAC,KR,KT,SP-001,allow,Rel-15,active,2025-10-23T06:00:00+09:00
ue capa,ue_max_bw_mhz,M100,450,5,APAC,KR,KT,SP-001,allow,100,active,2025-10-23T06:00:00+09:00
allow list,device_allowed,M101,440,8,EMEA,JP,SKT,SP-002,allow,false,active,2025-10-23T06:01:00+09:00
block list,blocked_reason,M101,440,8,EMEA,JP,SKT,SP-002,block,fraud,active,2025-10-23T06:01:00+09:00
rel features,nr_release,M101,440,8,EMEA,JP,SKT,SP-002,allow,Rel-16,active,2025-10-23T06:01:00+09:00
ue capa,ue_max_bw_mhz,M101,440,8,EMEA,JP,SKT,SP-002,allow,120,active,2025-10-23T06:01:00+09:00
allow list,device_allowed,M102,262,1,APAC,DE,LGU+,SP-003,allow,true,active,2025-10-23T06:02:00+09:00
block list,blocked_reason,M102,262,1,APAC,DE,LGU+,SP-003,block,roaming,active,2025-10-23T06:02:00+09:00
rel features,nr_release,M102,262,1,APAC,DE,LGU+,SP-003,allow,Rel-17,active,2025-10-23T06:02:00+09:00
ue capa,ue_max_bw_mhz,M102,262,1,APAC,DE,LGU+,SP-003,allow,140,active,2025-10-23T06:02:00+09:00
allow list,device_allowed,M103,208,6,EMEA,FR,NTT,SP-004,allow,false,active,2025-10-23T06:03:00+09:00
block list,blocked_reason,M103,208,6,EMEA,FR,NTT,SP-004,block,policy,active,2025-10-23T06:03:00+09:00
rel features,nr_release,M103,208,6,EMEA,FR,NTT,SP-004,allow,Rel-18,active,2025-10-23T06:03:00+09:00
ue capa,ue_max_bw_mhz,M103,208,6,EMEA,FR,NTT,SP-004,allow,160,active,2025-10-23T06:03:00+09:00
allow list,device_allowed,M104,450,3,APAC,KR,DOCOMO,SP-005,allow,true,active,2025-10-23T06:04:00+09:00
block list,blocked_reason,M104,450,3,APAC,KR,DOCOMO,SP-005,block,legacy,active,2025-10-23T06:04:00+09:00
rel features,nr_release,M104,450,3,APAC,KR,DOCOMO,SP-005,allow,Rel-15,active,2025-10-23T06:04:00+09:00
ue capa,ue_max_bw_mhz,M104,450,3,APAC,KR,DOCOMO,SP-005,allow,180,active,2025-10-23T06:04:00+09:00
allow list,device_allowed,M105,440,2,EMEA,JP,KDDI,SP-006,allow,false,active,2025-10-23T06:05:00+09:00
block list,blocked_reason,M105,440,2,EMEA,JP,KDDI,SP-006,block,,active,2025-10-23T06:05:00+09:00
rel features,nr_release,M105,440,2,EMEA,JP,KDDI,SP-006,allow,Rel-16,active,2025-10-23T06:05:00+09:00
ue capa,ue_max_bw_mhz,M105,440,2,EMEA,JP,KDDI,SP-006,allow,200,active,2025-10-23T06:05:00+09:00
allow list,device_allowed,M106,262,5,APAC,DE,Vodafone,SP-007,allow,true,active,2025-10-23T06:06:00+09:00
block list,blocked_reason,M106,262,5,APAC,DE,Vodafone,SP-007,block,fraud,active,2025-10-23T06:06:00+09:00
rel features,nr_release,M106,262,5,APAC,DE,Vodafone,SP-007,allow,Rel-17,active,2025-10-23T06:06:00+09:00
ue capa,ue_max_bw_mhz,M106,262,5,APAC,DE,Vodafone,SP-007,allow,100,active,2025-10-23T06:06:00+09:00
allow list,device_allowed,M107,208,8,EMEA,FR,Orange,SP-008,allow,false,active,2025-10-23T06:07:00+09:00
block list,blocked_reason,M107,208,8,EMEA,FR,Orange,SP-008,block,roaming,active,2025-10-23T06:07:00+09:00
rel features,nr_release,M107,208,8,EMEA,FR,Orange,SP-008,allow,Rel-18,active,2025-10-23T06:07:00+09:00
ue capa,ue_max_bw_mhz,M107,208,8,EMEA,FR,Orange,SP-008,allow,120,active,2025-10-23T06:07:00+09:00
allow list,device_allowed,M108,450,5,APAC,KR,DT,SP-009,allow,true,active,2025-10-23T06:08:00+09:00
block list,blocked_reason,M108,450,5,APAC,KR,DT,SP-009,block,policy,active,2025-10-23T06:08:00+09:00
rel features,nr_release,M108,450,5,APAC,KR,DT,SP-009,allow,Rel-15,active,2025-10-23T06:08:00+09:00
ue capa,ue_max_bw_mhz,M108,450,5,APAC,KR,DT,SP-009,allow,140,active,2025-10-23T06:08:00+09:00
allow list,device_allowed,M109,440,8,EMEA,JP,Free,SP-010,allow,false,active,2025-10-23T06:09:00+09:00
block list,blocked_reason,M109,440,8,EMEA,JP,Free,SP-010,block,legacy,active,2025-10-23T06:09:00+09:00
rel features,nr_release,M109,440,8,EMEA,JP,Free,SP-010,allow,Rel-16,active,2025-10-23T06:09:00+09:00
ue capa,ue_max_bw_mhz,M109,440,8,EMEA,JP,Free,SP-010,allow,160,active,2025-10-23T06:09:00+09:00
"""

# ë™ì¼ í´ë”ì— ìƒ˜í”Œ CSV ì €ì¥(í¸ì˜)
DEFAULT_CSV = pathlib.Path(__file__).with_name("fmw_sample_data.csv")
try:
    if not DEFAULT_CSV.exists():
        DEFAULT_CSV.write_text(SAMPLE_CSV, encoding="utf-8")
except Exception:
    pass

# ============== ê³µí†µ ìœ í‹¸/ìºì‹œ ==============
def _p(name: str, suffix: str) -> pathlib.Path:
    return DATA_DIR / f"{name}{suffix}"

def _meta_path(name: str) -> pathlib.Path:
    return _p(name, "._meta.json")

def _save_meta(name: str, **extra):
    meta = {"last_sync_kst": datetime.now(KST).isoformat(timespec="seconds")}
    meta.update(extra)
    _meta_path(name).write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

def read_meta(name: str) -> dict:
    p = _meta_path(name)
    if not p.exists():
        return {}
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return {}

def _load_df(name: str) -> pd.DataFrame:
    p = _p(name, ".parquet")
    return pd.read_parquet(p) if p.exists() else pd.DataFrame()

def _save_df(name: str, df: pd.DataFrame):
    _p(name, ".parquet").parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(_p(name, ".parquet"), index=False)

KEY_COLS = [
    "feature_group","feature_name","model_name","mcc","mnc",
    "region","country","operator","sp_type","mode"
]
REQ_COLS = KEY_COLS + ["value", "status", "updated_at"]

def ensure_cols(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    for c in REQ_COLS:
        if c not in df.columns:
            df[c] = None
    for c in df.columns:
        df[c] = df[c].astype(str)
    return df[REQ_COLS]

def df_keyed(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["_key"] = df[KEY_COLS].astype(str).agg("|".join, axis=1)
    return df

def diff_counts(old_df: pd.DataFrame, new_df: pd.DataFrame) -> Dict[str, int]:
    if old_df.empty:
        return {"added": len(new_df), "updated": 0, "removed": 0}
    old_k = df_keyed(old_df); new_k = df_keyed(new_df)
    old_keys = set(old_k["_key"]); new_keys = set(new_k["_key"])
    added = len(new_keys - old_keys)
    removed = len(old_keys - new_keys)
    merged = new_k.merge(old_k[["_key","value","status"]], on="_key", how="left", suffixes=("", "_old"))
    updated = (merged["value"] != merged["value_old"]) | (merged["status"] != merged["status_old"])
    return {"added": added, "updated": int(updated.sum()), "removed": removed}

# ============== ìµœê·¼ ë³€ê²½ ìŠ¤ëƒ…ìƒ·/ì¶”ì´ ==============
RECENT_CHANGES_FILE = DATA_DIR / "recent_changes.csv"

def _append_recent_changes(df_new: pd.DataFrame) -> None:
    cols_order = [
        "ts_kst","group","feature","action",
        "model_name","operator","region","country","mcc","mnc","sp_type","mode",
        "before_value","after_value","before_status","after_status","_key"
    ]
    for c in cols_order:
        if c not in df_new.columns:
            df_new[c] = ""
    if RECENT_CHANGES_FILE.exists():
        base = pd.read_csv(RECENT_CHANGES_FILE, dtype=str, keep_default_na=False)
        out = pd.concat([df_new[cols_order], base], ignore_index=True)
    else:
        out = df_new[cols_order]
    out = out.drop_duplicates(subset=["ts_kst","_key","action"], keep="first")
    out = out.sort_values("ts_kst", ascending=False).head(5000)
    out.to_csv(RECENT_CHANGES_FILE, index=False)

def snapshot_changes(cache_name: str, old_df: pd.DataFrame, new_df: pd.DataFrame) -> dict:
    now_kst = datetime.now(KST).isoformat(timespec="seconds")
    g, f = cache_name.replace("records__", "").split("__", 1)
    oldk = df_keyed(ensure_cols(old_df))
    newk = df_keyed(ensure_cols(new_df))
    old_keys = set(oldk["_key"]); new_keys = set(newk["_key"]); both_keys = old_keys & new_keys

    added = newk[newk["_key"].isin(new_keys - old_keys)].copy()
    added["action"] = "ADD"; added["before_value"] = ""; added["before_status"] = ""
    added["after_value"] = added["value"]; added["after_status"] = added["status"]

    removed = oldk[oldk["_key"].isin(old_keys - new_keys)].copy()
    removed["action"] = "REM"; removed["before_value"] = removed["value"]; removed["before_status"] = removed["status"]
    removed["after_value"] = ""; removed["after_status"] = ""

    merged = newk[newk["_key"].isin(both_keys)].merge(
        oldk[["_key","value","status"]], on="_key", how="left", suffixes=("", "_old")
    )
    upd_mask = (merged["value"] != merged["value_old"]) | (merged["status"] != merged["status_old"])
    updated = merged.loc[upd_mask].copy()
    updated["action"] = "UPD"
    updated["before_value"] = updated["value_old"].fillna("")
    updated["before_status"] = updated["status_old"].fillna("")
    updated["after_value"] = updated["value"]
    updated["after_status"] = updated["status"]

    def _decorate(df):
        if df.empty: return df
        df["ts_kst"] = now_kst; df["group"] = g; df["feature"] = f
        return df[[
            "ts_kst","group","feature","action",
            "model_name","operator","region","country","mcc","mnc","sp_type","mode",
            "before_value","after_value","before_status","after_status","_key"
        ]]

    out_df = pd.concat([_decorate(added), _decorate(updated), _decorate(removed)], ignore_index=True)
    if not out_df.empty:
        _append_recent_changes(out_df)

    return {"added": len(added), "updated": len(updated), "removed": len(removed)}

def load_recent_changes(limit: int = 200) -> pd.DataFrame:
    if not RECENT_CHANGES_FILE.exists():
        return pd.DataFrame(columns=[
            "ts_kst","group","feature","action",
            "model_name","operator","region","country","mcc","mnc","sp_type","mode",
            "before_value","after_value","before_status","after_status","_key"
        ])
    df = pd.read_csv(RECENT_CHANGES_FILE, dtype=str, keep_default_na=False)
    return df.sort_values("ts_kst", ascending=False).head(limit)

def today_change_counts(df_rc: pd.DataFrame) -> int:
    if df_rc.empty: return 0
    # ë‚ ì§œ ë¹„êµëŠ” Asia/Seoul ê¸°ì¤€ ë¬¸ìì—´ date ë¹„êµ
    today = datetime.now(KST).date().isoformat()
    return int((pd.to_datetime(df_rc["ts_kst"]).dt.tz_localize("Asia/Seoul", nonexistent="shift_forward", ambiguous="NaT", errors="coerce").dt.date.astype(str) == today).sum())

def trend7(df_rc: pd.DataFrame) -> pd.DataFrame:
    if df_rc.empty:
        return pd.DataFrame({"date": [], "ADD": [], "UPD": [], "REM": []})
    df = df_rc.copy()
    df["date"] = pd.to_datetime(df["ts_kst"]).dt.tz_localize("Asia/Seoul", nonexistent="shift_forward", ambiguous="NaT", errors="coerce").dt.date
    pv = df.pivot_table(index="date", columns="action", values="_key", aggfunc="count", fill_value=0).reset_index()
    # 7ì¼ë§Œ
    pv = pv.sort_values("date", ascending=True).tail(7)
    for c in ("ADD","UPD","REM"):
        if c not in pv.columns: pv[c] = 0
    return pv[["date","ADD","UPD","REM"]]

# ============== ë°ì´í„° ì†ŒìŠ¤: ìƒ˜í”Œ vs API ==============
def _get(path: str, params: Optional[Dict[str, Any]] = None):
    url = f"{API_BASE}/{path.lstrip('/')}"
    headers = {"X-API-KEY": API_KEY} if API_KEY else {}
    r = requests.get(url, params=params or {}, headers=headers, verify=VERIFY_SSL, timeout=120)
    r.raise_for_status()
    return r.json()

@st.cache_data
def sample_df() -> pd.DataFrame:
    return pd.read_csv(io.StringIO(SAMPLE_CSV), dtype=str, keep_default_na=False)

def list_feature_groups_sample() -> List[Dict[str, Any]]:
    df = sample_df()
    groups = sorted(df["feature_group"].dropna().unique().tolist())
    return [{"name": g} for g in groups]

def list_features_sample(group_name: str) -> List[Dict[str, Any]]:
    df = sample_df()
    feats = sorted(df.loc[df["feature_group"].eq(group_name), "feature_name"].dropna().unique().tolist())
    return [{"name": f} for f in feats]

def list_feature_records_sample(group_name: str, feature_name: str, **filters) -> List[Dict[str, Any]]:
    df = sample_df()
    q = df[(df["feature_group"] == group_name) & (df["feature_name"] == feature_name)].copy()
    # ì •í™• ë§¤ì¹­ í•„í„°
    for k in ("region","country","mcc","mnc","mode","sp_type"):
        v = filters.get(k)
        if v:
            q = q[q[k].astype(str) == str(v)]
    # í¬í•¨ê²€ìƒ‰
    for k in ("model","operator"):
        v = filters.get(k)
        if v:
            col = "model_name" if k == "model" else "operator"
            q = q[q[col].str.contains(str(v), case=False, na=False)]
    return q.to_dict(orient="records")

def list_feature_groups_api() -> List[Dict[str, Any]]:
    return _get(PATH_GROUPS)

def list_features_api(group_name: str) -> List[Dict[str, Any]]:
    return _get(PATH_FEATURES, {"group": group_name})

def list_feature_records_api(group_name: str, feature_name: str, **filters) -> List[Dict[str, Any]]:
    # PRD: GET-only, params ê¸°ë°˜ í•„í„°
    params = {"group": group_name, "feature": feature_name}
    for k in ("model","operator","region","country","mcc","mnc","mode","sp_type","page","page_size"):
        v = filters.get(k)
        if v not in (None, "", []):
            params[k] = v
    # /feature-records/ ë˜ëŠ” /long-records/ ì¤‘ ì„ íƒ
    use_long = bool(filters.get("_use_long", False))
    path = PATH_LONG if use_long else PATH_RECORDS
    data = _get(path, params)
    # DRF í˜ì´ì§• ëŒ€ì‘
    if isinstance(data, dict) and "results" in data:
        return data["results"]
    return data

if USE_SAMPLE == "1":
    list_feature_groups = list_feature_groups_sample
    list_features = list_features_sample
    list_feature_records = list_feature_records_sample
else:
    list_feature_groups = list_feature_groups_api
    list_features = list_features_api
    list_feature_records = list_feature_records_api

# ============== ë™ê¸°í™”(06:00 ìë™) ==============
def sync_all() -> Dict[str, Any]:
    summary = {"groups": 0, "features": 0, "records_added": 0, "records_updated": 0, "records_removed": 0}
    groups = list_feature_groups() or []
    summary["groups"] = len(groups)

    for g in groups:
        gname = g.get("name")
        feats = list_features(gname) or []
        summary["features"] += len(feats)
        for f in feats:
            fname = f.get("name")
            # long ë·° ì‚¬ìš© ì‹œ íŒŒë¼ë¯¸í„°ë¡œ í™œì„±í™” ê°€ëŠ¥(í˜„ì¬ëŠ” ê¸°ë³¸ off)
            new_rows = list_feature_records(gname, fname, _use_long=False)
            new_df = ensure_cols(pd.DataFrame(new_rows))
            cache_name = f"records__{gname}__{fname}"
            old_df = _load_df(cache_name)

            dcnt = diff_counts(old_df, new_df)
            _ = snapshot_changes(cache_name, old_df, new_df)

            _save_df(cache_name, new_df)
            _save_meta(cache_name, **dcnt)

            summary["records_added"]   += dcnt["added"]
            summary["records_updated"] += dcnt["updated"]
            summary["records_removed"] += dcnt["removed"]

    _save_meta("index", **summary)
    return summary

def maybe_daily_sync(sync_fn):
    now = datetime.now(KST)
    today6 = now.replace(hour=6, minute=0, second=0, microsecond=0)
    last_str = read_meta("index").get("last_sync_kst")
    last_dt = None
    if last_str:
        try:
            last_dt = datetime.fromisoformat(last_str)
        except Exception:
            last_dt = None
    need = (not last_dt) or (now >= today6 and last_dt.date() != now.date())
    if need:
        try:
            sync_fn()
        except Exception as e:
            st.warning(f"ìë™ ë™ê¸°í™” ì‹¤íŒ¨: {e}")

def start_scheduler():
    if BackgroundScheduler:
        sched = BackgroundScheduler()
        sched.add_job(sync_all, "cron", hour=6, minute=0, timezone="Asia/Seoul", id="daily_sync", replace_existing=True)
        sched.start()
        return "APScheduler(06:00)"
    else:
        maybe_daily_sync(sync_all)
        return "Fallback(ì•± ì—´ë¦´ ë•Œ í•˜ë£¨ 1íšŒ)"

# ============== ë„ìš°ë¯¸: ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ==============
def df_to_excel_bytes(df: pd.DataFrame) -> Optional[bytes]:
    if df.empty: return None
    if not XLSX_AVAILABLE: return None
    buf = io.BytesIO()
    with pd.ExcelWriter(buf, engine="xlsxwriter") as writer:
        df.to_excel(writer, index=False, sheet_name="data")
    return buf.getvalue()

# ============== UI (ëª¨ë°”ì¼ ìµœì í™”) ==============
st.set_page_config(page_title="FMW ë°ì´í„° ë§¤ë‹ˆì €", layout="wide", page_icon="ğŸ—‚ï¸")
st.markdown("""
    <style>
    .big-btn button {padding: 0.6rem 1rem; font-size: 1.05rem; border-radius: 12px;}
    .metric-small .stMetric {padding: .2rem .6rem;}
    .chip {display:inline-block;padding:.2rem .5rem;margin-right:.3rem;border-radius:999px;background:#eee;}
    @media (max-width: 640px){
      .stTabs [data-baseweb="tab-list"] {gap:.25rem;}
    }
    </style>
""", unsafe_allow_html=True)

st.title("FMW ë°ì´í„° ë§¤ë‹ˆì € Â· UX v3")
st.caption("GET-only API Â· 06:00 ìë™ ë™ê¸°í™” Â· ì˜¤ë²„ë·°/íƒìƒ‰/ë³€ê²½ ì´ë ¥ Â· CSV/Excel ë‹¤ìš´ë¡œë“œ")

status_msg = start_scheduler()
st.info(f"ë™ê¸°í™” ìƒíƒœ: {status_msg} | ëª¨ë“œ: {'ìƒ˜í”Œ' if USE_SAMPLE=='1' else 'ì‹¤API'}")

# ì´ˆê¸° ìºì‹œ ì—†ìœ¼ë©´ 1íšŒ ë™ê¸°í™”
if not _meta_path("index").exists():
    try:
        sync_all()
    except Exception as e:
        st.warning(f"ì´ˆê¸° ë™ê¸°í™” ì‹¤íŒ¨: {e}")

# --- ì‚¬ì´ë“œë°”: ê·¸ë£¹/í”¼ì²˜/í•„í„° ---
st.sidebar.header("íƒìƒ‰ & í•„í„°")
try:
    g_list = list_feature_groups() or []
    group_names = [g.get("name") for g in g_list]
except Exception as e:
    group_names = []
    st.sidebar.error(f"ê·¸ë£¹ ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")

sel_group = st.sidebar.selectbox("Feature Group", [""] + group_names, index=1 if group_names else 0)
feat_names: List[str] = []
if sel_group:
    try:
        f_list = list_features(sel_group) or []
        feat_names = [f.get("name") for f in f_list]
    except Exception as e:
        st.sidebar.error(f"í”¼ì²˜ ëª©ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")
sel_feature = st.sidebar.selectbox("Feature", [""] + feat_names, index=1 if feat_names else 0)

st.sidebar.subheader("ìƒì„¸ í•„í„°")
col_a, col_b = st.sidebar.columns(2)
with col_a:
    mode_eq     = st.selectbox("Mode", ["", "allow", "block"], index=0)
    region_eq   = st.text_input("Region =")
    mcc_eq      = st.text_input("MCC =")
with col_b:
    country_eq  = st.text_input("Country =")
    mnc_eq      = st.text_input("MNC =")
    sp_eq       = st.text_input("SP Type =")

model_like    = st.sidebar.text_input("Model í¬í•¨ê²€ìƒ‰")
operator_like = st.sidebar.text_input("Operator í¬í•¨ê²€ìƒ‰")

c1, c2, c3 = st.sidebar.columns(3)
with c1:
    run_query = st.button("ë°ì´í„° ì¡°íšŒ", type="primary", use_container_width=True)
with c2:
    reset = st.button("í•„í„° ì´ˆê¸°í™”", use_container_width=True)
with c3:
    use_long = st.checkbox("Long ë·° ì‚¬ìš©", value=False, help="/long-records/ ì‚¬ìš©(ê°€ëŠ¥ ì‹œ)")
if reset:
    mode_eq = region_eq = mcc_eq = country_eq = mnc_eq = sp_eq = model_like = operator_like = ""
    st.experimental_rerun()

# --- íƒ­ êµ¬ì„± ---
tab_ov, tab_explore, tab_changes = st.tabs(["ğŸ“Š ì˜¤ë²„ë·°", "ğŸ§­ íƒìƒ‰", "ğŸ•’ ë³€ê²½ ì´ë ¥"])

# ===== ì˜¤ë²„ë·° =====
with tab_ov:
    meta = read_meta("index")
    idx_last = meta.get("last_sync_kst", "-")
    added = int(meta.get("records_added", 0) or 0)
    updated = int(meta.get("records_updated", 0) or 0)
    removed = int(meta.get("records_removed", 0) or 0)

    rc_base = load_recent_changes(limit=1000)
    today_cnt = today_change_counts(rc_base)

    # ì´ ë ˆì½”ë“œ(ê°„ë‹¨ ì‚°ì¶œ): ìºì‹œì— ì €ì¥ëœ ëª¨ë“  parquet í•©
    # ì‹¤ì œ ì´ê³„ëŠ” ë°±ì—”ë“œ ì§‘ê³„ APIê°€ ì´ìƒì ì´ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ìºì‹œ í•©ìœ¼ë¡œ ëŒ€ì²´
    total_records = 0
    for p in DATA_DIR.glob("records__*__.parquet"):
        try:
            total_records += len(pd.read_parquet(p))
        except Exception:
            pass

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("ì´ ë ˆì½”ë“œ(ìºì‹œ)", total_records)
    col2.metric("ê¸ˆì¼ ë³€ê²½(ê±´)", today_cnt)
    col3.metric("ì‹¤íŒ¨ ëŸ°(ì˜¤ëŠ˜)", "N/A")  # GET-only í™˜ê²½ì—ì„œëŠ” ì¶”ì  ê³¤ë€ â†’ ë°±ì—”ë“œ run API í•„ìš”
    col4.metric("ë§ˆì§€ë§‰ ë™ê¸°í™”(KST)", idx_last)

    st.markdown("#### ìµœê·¼ 7ì¼ ë³€ê²½ ì¶”ì´")
    tr = trend7(rc_base)
    if tr.empty:
        st.info("ì¶”ì´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë™ê¸°í™” í›„ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")
    else:
        st.line_chart(tr.set_index("date")[["ADD","UPD","REM"]])

    # ì„ íƒ í•­ëª© í˜„í™©
    st.markdown("#### ì„ íƒ í•­ëª© í˜„í™©")
    if sel_group and sel_feature:
        df = _load_df(f"records__{sel_group}__{sel_feature}")
        if not df.empty:
            df = ensure_cols(df)
            total = len(df)
            allow_cnt = int((df.get("mode","").astype(str)=="allow").sum())
            block_cnt = int((df.get("mode","").astype(str)=="block").sum())
            cc1, cc2, cc3 = st.columns(3)
            cc1.metric("ì´ ë ˆì½”ë“œ", total)
            cc2.metric("allow", allow_cnt)
            cc3.metric("block", block_cnt)
            st.caption(f"<span class='chip'>group: {sel_group}</span> <span class='chip'>feature: {sel_feature}</span>", unsafe_allow_html=True)
        else:
            st.info("ì„ íƒí•œ ê·¸ë£¹/í”¼ì²˜ì˜ ìºì‹œê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ì¢Œì¸¡ì—ì„œ ê·¸ë£¹ê³¼ í”¼ì²˜ë¥¼ ì„ íƒí•˜ë©´ í•´ë‹¹ í˜„í™©ì„ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤.")

# ===== íƒìƒ‰ =====
def load_cached_records(g: str, f: str) -> pd.DataFrame:
    if not (g and f): return pd.DataFrame()
    return ensure_cols(_load_df(f"records__{g}__{f}"))

with tab_explore:
    st.markdown("#### ì¡°ê±´ë³„ ì¡°íšŒ")
    if run_query and sel_group and sel_feature:
        df = load_cached_records(sel_group, sel_feature)
        if df.empty:
            st.info("ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤. 06:00 ìë™ ë™ê¸°í™” ì´í›„ í™•ì¸í•˜ì„¸ìš”.")
        else:
            q = df.copy()
            # ì •í™• ë§¤ì¹­
            if mode_eq:       q = q[q["mode"] == mode_eq]
            if region_eq:     q = q[q["region"] == region_eq]
            if country_eq:    q = q[q["country"] == country_eq]
            if mcc_eq:        q = q[q["mcc"] == str(mcc_eq)]
            if mnc_eq:        q = q[q["mnc"] == str(mnc_eq)]
            if sp_eq:         q = q[q["sp_type"] == sp_eq]
            # í¬í•¨ê²€ìƒ‰
            if model_like:    q = q[q["model_name"].str.contains(model_like, case=False, na=False)]
            if operator_like: q = q[q["operator"].str.contains(operator_like, case=False, na=False)]

            st.caption(f"ğŸ” ê²°ê³¼ {len(q)}ê±´")
            st.dataframe(q, use_container_width=True, height=420)

            # ë‹¤ìš´ë¡œë“œ
            st.download_button("CSV ë‹¤ìš´ë¡œë“œ", q.to_csv(index=False).encode("utf-8"),
                               file_name=f"{sel_group}_{sel_feature}_filtered.csv", mime="text/csv", type="primary")
            xbytes = df_to_excel_bytes(q)
            if xbytes:
                st.download_button("Excel(.xlsx) ë‹¤ìš´ë¡œë“œ", xbytes,
                                   file_name=f"{sel_group}_{sel_feature}_filtered.xlsx",
                                   mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
            else:
                st.caption("ğŸ’¡ Excel ë‹¤ìš´ë¡œë“œëŠ” 'xlsxwriter' ì„¤ì¹˜ ì‹œ í™œì„±í™”ë©ë‹ˆë‹¤.")
    else:
        st.info("ì¢Œì¸¡ì—ì„œ ì¡°ê±´ì„ ì„ íƒí•˜ê³  **ë°ì´í„° ì¡°íšŒ**ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

# ===== ë³€ê²½ ì´ë ¥ =====
with tab_changes:
    st.markdown("#### ìµœê·¼ ë³€ê²½ (ìŠ¤ëƒ…ìƒ·)")
    rc = load_recent_changes(limit=500)
    c1, c2, c3 = st.columns(3)
    action = c1.selectbox("ì•¡ì…˜", ["ALL","ADD","UPD","REM"], index=0)
    if sel_group:
        rc = rc[rc["group"] == sel_group] if not rc.empty else rc
    if sel_feature:
        rc = rc[rc["feature"] == sel_feature] if not rc.empty else rc
    if action != "ALL" and not rc.empty:
        rc = rc[rc["action"] == action]

    if rc.empty:
        st.info("ìµœê·¼ ë³€ê²½ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. 06:00 ìë™ ë™ê¸°í™” ì´í›„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        # ì´ëª¨ì§€ ì¹©
        def emoji(a):
            return {"ADD":"ğŸŸ¢ ADD","UPD":"ğŸŸ¡ UPD","REM":"ğŸ”´ REM"}.get(a, a)
        rc = rc.copy()
        rc["action"] = rc["action"].map(emoji)
        st.dataframe(
            rc[["ts_kst","group","feature","action","model_name","operator","region","country","mcc","mnc","sp_type","mode","before_value","after_value"]],
            use_container_width=True, height=420
        )
        st.download_button("ìµœê·¼ ë³€ê²½ CSV ë‹¤ìš´ë¡œë“œ", rc.to_csv(index=False).encode("utf-8"),
                           file_name="recent_changes.csv", mime="text/csv", type="primary")
