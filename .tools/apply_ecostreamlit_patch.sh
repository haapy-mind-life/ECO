#!/usr/bin/env bash
set -euo pipefail

BASE_BRANCH="${BASE_BRANCH:-main}"
FEATURE_BRANCH="feature/streamlit-ux-$(date +%Y%m%d)"

echo "==> Git ìƒíƒœ í™•ì¸"
git status >/dev/null

echo "==> ì›ê²© ìµœì‹  ë™ê¸°í™”"
git fetch origin --prune

echo "==> ìž‘ì—… ë¸Œëžœì¹˜ ìƒì„±: ${FEATURE_BRANCH} (base=${BASE_BRANCH})"
git checkout -B "${FEATURE_BRANCH}" "origin/${BASE_BRANCH}"

echo "==> ë””ë ‰í„°ë¦¬ ì¤€ë¹„"
mkdir -p .streamlit app/core app/views app/data scripts .github/workflows

echo "==> requirements.txt ìž‘ì„±"
cat > requirements.txt <<'EOF_REQ'
streamlit>=1.33,<2.0
pandas>=2.0,<3.0
requests>=2.31
pyarrow>=14.0
# optional: ìœ ì‚¬ê²€ìƒ‰/ì¶”ì²œì´ í•„ìš”í•˜ë©´ ì‚¬ìš©
# rapidfuzz>=3.9
EOF_REQ

echo "==> .streamlit/config.toml ìž‘ì„±"
cat > .streamlit/config.toml <<'EOF_TOML'
[server]
port = 8501
baseUrlPath = "/fmw"
enableXsrfProtection = true
enableCORS = false
EOF_TOML

echo "==> main.py ìž‘ì„±"
cat > main.py <<'EOF_MAIN'
from __future__ import annotations
from typing import List, Tuple
import os, time, requests
import pandas as pd
import streamlit as st

from app.core.navigation import get_pages
from app.data.sample_features import distinct_values, load_feature_dataframe
from app.data.data_manager import DataManager
from app.views.state import FilterState

# ---------------- URL ìƒíƒœ ë™ê¸°í™” ---------------- #

def _init_session_state() -> FilterState:
    if "filter_state" not in st.session_state:
        st.session_state.filter_state = FilterState()
    return st.session_state.filter_state

@st.cache_data(ttl=86400)  # Cloud: í•˜ë£¨ 1íšŒ ë™ê¸°í™”
def _cloud_fetch_feature1(api_base: str, verify_ssl: bool, token: str | None) -> Tuple[pd.DataFrame, int]:
    url = f"{api_base.rstrip('/')}/api/feature1/"
    headers = {}
    if token:
        headers["Authorization"] = f"Bearer {token}"
    r = requests.get(url, timeout=20, verify=verify_ssl, headers=headers)
    r.raise_for_status()
    data = r.json()
    if isinstance(data, dict) and "results" in data:
        data = data["results"]
    df = pd.DataFrame(data)
    last = int(time.time())
    return df, last

def _read_qs_list(name: str) -> List[str]:
    qs = st.experimental_get_query_params()
    vals = qs.get(name, [])
    if not vals:
        return []
    merged = []
    for v in vals:
        merged.extend([x.strip() for x in v.split(",") if x.strip()])
    return merged

def _sync_qs_from_state(state: FilterState) -> None:
    st.experimental_set_query_params(
        models=state.models or None,
        feature_groups=state.feature_groups or None,
    )

# ---------------- ë©”ì¸ ì•± ---------------- #

def _render_sidebar(filter_state: FilterState) -> str:
    pages = get_pages()
    labels = [page.label for page in pages]

    st.sidebar.title("ðŸ§­ ë‚´ë¹„ê²Œì´ì…˜")
    selected_label = st.sidebar.radio("ë©”ì¸ ê¸°ëŠ¥", labels, key="main_navigation")

    selected_page = next(p for p in pages if p.label == selected_label)
    if selected_page.description:
        st.sidebar.caption(selected_page.description)

    # ì²« ì§„ìž… ì‹œ URLì˜ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ë¥¼ ê³µí†µ í•„í„°ì— ë°˜ì˜
    if not (filter_state.models or filter_state.feature_groups):
        from_qs_models = _read_qs_list("models")
        from_qs_groups = _read_qs_list("feature_groups")
        if from_qs_models:
            filter_state.models = from_qs_models
        if from_qs_groups:
            filter_state.feature_groups = from_qs_groups
        _sync_qs_from_state(filter_state)

    st.sidebar.divider()
    st.sidebar.subheader("ê³µí†µ í•„í„° (ë¨¼ì € ì„ íƒ ê¶Œìž¥)")

    filter_state.models = st.sidebar.multiselect(
        "ëª¨ë¸",
        distinct_values("model"),
        default=filter_state.models,
        key="sidebar_models",
        placeholder="ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
    )
    filter_state.feature_groups = st.sidebar.multiselect(
        "FEATURE GROUP",
        distinct_values("feature_group"),
        default=filter_state.feature_groups,
        key="sidebar_feature_groups",
        placeholder="FEATURE GROUPì„ ì„ íƒí•˜ì„¸ìš”",
    )

    st.sidebar.caption("ì„ íƒí•œ í•­ëª©ì€ ëª¨ë“  í™”ë©´ì˜ ë°ì´í„° í•„í„°ì— ë°”ë¡œ ë°˜ì˜ë©ë‹ˆë‹¤.")

    st.sidebar.divider()
    st.sidebar.subheader("í™•ìž¥ ì•±")
    st.sidebar.caption("ì¶”ê°€ ê¸°ëŠ¥(ì±—ë´‡, ìžë™í™” ë“±)ì„ ì—°ê²°í•  ì˜ì—­ìž…ë‹ˆë‹¤.")

    _sync_qs_from_state(filter_state)
    return selected_label


def main():
    st.set_page_config(page_title="Feature Monitoring Portal", layout="wide")

    # ëŸ°íƒ€ìž„ ëª¨ë“œ ê°ì§€
    runtime_mode = st.secrets.get("RUNTIME_MODE", os.getenv("RUNTIME_MODE", "cloud")).lower()
    api_base = st.secrets.get("API_BASE", os.getenv("API_BASE", "https://10.x.x.x"))
    verify_ssl = bool(str(st.secrets.get("VERIFY_SSL", os.getenv("VERIFY_SSL", "false"))).lower() in ("1","true","yes"))
    token = st.secrets.get("API_ACCESS_TOKEN", os.getenv("API_ACCESS_TOKEN", None))
    data_dir = os.getenv("DATA_DIR", "./_cache")

    # ë°ì´í„°í”„ë ˆìž„ ë¡œë”© (Cloud â†” On-prem ìžë™ ì „í™˜)
    if runtime_mode == "cloud":
        try:
            dataframe, last = _cloud_fetch_feature1(api_base, verify_ssl, token)
            st.session_state["data_manager"] = None
            st.session_state["last_sync_txt"] = time.strftime("%Y-%m-%d %H:%M:%S KST", time.gmtime(last + 9*3600))
        except Exception:
            dataframe = load_feature_dataframe()  # ìƒ˜í”Œ í´ë°±
            st.session_state["last_sync_txt"] = "ìƒ˜í”Œ ë°ì´í„°(Cloud API ë¯¸ì ‘ì†)"
    else:
        dm = DataManager(api_base=api_base, data_dir=data_dir, verify_ssl=verify_ssl)
        st.session_state["data_manager"] = dm
        dm.refresh_if_stale("feature1", max_age_hours=24)
        dataframe = dm.load("feature1")
        if dataframe.empty:
            dataframe = load_feature_dataframe()

    # ìŠ¤í‚¤ë§ˆ ì •í•©ì„± ê²½ê³ (ê°œë°œìž íŽ¸ì˜)
    expected = {"feature_group","feature_name","model","mcc","mnc","region","country","operator","status","last_updated","visualization_group"}
    missing = expected - set(dataframe.columns)
    if missing:
        st.warning(f"ë°ì´í„° ì»¬ëŸ¼ ëˆ„ë½: {sorted(missing)} Â· ì‹¤ì œ ì—°ë™ ì „ ìŠ¤í‚¤ë§ˆë¥¼ ë§žì¶°ì£¼ì„¸ìš”.")

    filter_state = _init_session_state()
    selected_label = _render_sidebar(filter_state)

    pages = {page.label: page.renderer for page in get_pages()}
    updated_state = pages[selected_label](filter_state, dataframe)
    st.session_state.filter_state = updated_state
    _sync_qs_from_state(updated_state)


if __name__ == "__main__":
    main()
EOF_MAIN

echo "==> app/core/navigation.py ìž‘ì„±"
cat > app/core/navigation.py <<'EOF_NAV'
from __future__ import annotations
from dataclasses import dataclass
from typing import Callable, Iterable, Sequence
import pandas as pd

from app.views.home import render_home
from app.views.state import FilterState
from app.views.visualization import render_visualization

RenderFn = Callable[[FilterState, pd.DataFrame], FilterState]

@dataclass(frozen=True)
class Page:
    label: str
    renderer: RenderFn
    description: str | None = None

_DEFAULT_PAGES: tuple[Page, ...] = (
    Page("ðŸ  í™ˆ", render_home, "í•„í„° ê¸°ë°˜ í™ˆ ëŒ€ì‹œë³´ë“œ"),
    Page("ðŸ“ˆ ì‹œê°í™”", render_visualization, "ì„ íƒëœ FEATURE GROUP ê¸°ì¤€ ê°€ì´ë“œ"),
)

def get_pages(extra_pages: Iterable[Page] | None = None) -> Sequence[Page]:
    if not extra_pages:
        return _DEFAULT_PAGES
    return _DEFAULT_PAGES + tuple(extra_pages)
EOF_NAV

echo "==> app/views/state.py ìž‘ì„±"
cat > app/views/state.py <<'EOF_STATE'
from __future__ import annotations
from dataclasses import dataclass, field
from typing import List

@dataclass
class FilterState:
    models: List[str] = field(default_factory=list)
    feature_groups: List[str] = field(default_factory=list)
    mcc: List[str] = field(default_factory=list)
    mnc: List[str] = field(default_factory=list)
    regions: List[str] = field(default_factory=list)
    countries: List[str] = field(default_factory=list)
    operators: List[str] = field(default_factory=list)
    features: List[str] = field(default_factory=list)

    def apply(self, dataframe):
        df = dataframe
        selectors = {
            "model": self.models,
            "feature_group": self.feature_groups,
            "mcc": self.mcc,
            "mnc": self.mnc,
            "region": self.regions,
            "country": self.countries,
            "operator": self.operators,
            "feature_name": self.features,
        }
        for column, selected in selectors.items():
            if selected and column in df.columns:
                df = df[df[column].isin(selected)]
        return df

    def active_visualization_groups(self, dataframe) -> List[str]:
        filtered = self.apply(dataframe)
        if "visualization_group" not in filtered.columns:
            return []
        groups = filtered["visualization_group"].dropna().unique().tolist()
        groups.sort()
        return groups
EOF_STATE

echo "==> app/views/home.py ìž‘ì„±"
cat > app/views/home.py <<'EOF_HOME'
from __future__ import annotations
import pandas as pd
import streamlit as st
from app.data.data_manager import DataManager
from app.data.sample_features import distinct_values, FEATURE_GROUP_SCHEMA
from app.views.state import FilterState
import re

def _mccmnc_ok(s: str) -> bool:
    return bool(re.fullmatch(r"\\d{3}-\\d{2}", s.strip()))

def _multiselect(label: str, options, default, key: str):
    return st.multiselect(label, options, default=default, key=key, placeholder="ê²€ìƒ‰í•˜ê±°ë‚˜ ê°’ì„ ìž…ë ¥í•˜ì„¸ìš”")


def render_home(filter_state: FilterState, dataframe: pd.DataFrame):
    st.title("ðŸ“Š Feature Monitoring Home")

    # ë§ˆì§€ë§‰ DB ì‹±í¬ ì‹œê°
    last_sync_txt = "-"
    dm: DataManager | None = st.session_state.get("data_manager")
    if dm:
        last = dm.last_sync_at("feature1")
        last_sync_txt = dm.format_last_sync(last)
    else:
        last_sync_txt = st.session_state.get("last_sync_txt", "-")
    st.caption(f"ë§ˆì§€ë§‰ DB ì‹±í¬: **{last_sync_txt}** Â· ì‹±í¬ ì£¼ê¸°: **ë§¤ì¼ 1íšŒ** Â· ì¡°íšŒ ì „ìš©")

    # ê·¸ë£¹ ì¸ì§€í˜•: ì„ íƒëœ ê·¸ë£¹ì— ë”°ë¼ ì‚¬ìš© ì°¨ì› ì œì–´
    # ë‹¨ì¼ ê·¸ë£¹ ì„ íƒ ì‹œ í•´ë‹¹ ìŠ¤í‚¤ë§ˆ, ë³µìˆ˜ ì„ íƒì´ë©´ ëª¨ë“  ì°¨ì› ë…¸ì¶œ(ë³´ìˆ˜ì )
    dims = {"region","country","operator","mcc_mnc"}
    if filter_state.feature_groups and len(filter_state.feature_groups) == 1:
        g = filter_state.feature_groups[0]
        dims = set(FEATURE_GROUP_SCHEMA.get(g, {}).get("dims", list(dims)))

    st.subheader("ìƒì„¸ í•„í„°")
    col1, col2, col3 = st.columns(3)

    with col1:
        if "mcc_mnc" in dims:
            # MCC/MNCëŠ” ì›ë³¸ ì»¬ëŸ¼ì´ mcc,mncë¡œ ì˜¬ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ ìž…ë ¥ì€ NNN-NN
            mccmnc = st.text_input("MCC-MNC(ì •í™•ížˆ: NNN-NN)", key="home_mccmnc", placeholder="ì˜ˆ: 450-05")
            if mccmnc and not _mccmnc_ok(mccmnc):
                st.error("MCC-MNC í˜•ì‹ì€ NNN-NN (ì˜ˆ: 450-05)")
                st.stop()
            # í•„í„°ì— ë°˜ì˜: mcc, mnc ë¶„í•´
            if mccmnc:
                try:
                    mcc, mnc = mccmnc.split("-")
                    filter_state.mcc = [mcc]
                    filter_state.mnc = [mnc]
                except Exception:
                    pass
        else:
            filter_state.mcc, filter_state.mnc = [], []

    with col2:
        if "region" in dims:
            filter_state.regions = _multiselect("ì§€ì—­", distinct_values("region"), filter_state.regions, "home_region")
        else:
            filter_state.regions = []
        if "country" in dims:
            filter_state.countries = _multiselect("êµ­ê°€", distinct_values("country"), filter_state.countries, "home_country")
        else:
            filter_state.countries = []

    with col3:
        if "operator" in dims:
            filter_state.operators = _multiselect("ì‚¬ì—…ìž", distinct_values("operator"), filter_state.operators, "home_operator")
        else:
            filter_state.operators = []
        filter_state.features = _multiselect("FEATURE", distinct_values("feature_name"), filter_state.features, "home_feature")

    filtered_df = filter_state.apply(dataframe).copy()

    # KPI
    st.divider()
    k1,k2,k3,k4 = st.columns(4)
    with k1: st.metric("í–‰ ìˆ˜", len(filtered_df))
    with k2: st.metric("ëª¨ë¸ ìˆ˜", int(filtered_df["model"].nunique()) if not filtered_df.empty and "model" in filtered_df else 0)
    with k3: st.metric("FEATURE ìˆ˜", int(filtered_df["feature_name"].nunique()) if not filtered_df.empty and "feature_name" in filtered_df else 0)
    with k4: st.metric("ì‚¬ì—…ìž ìˆ˜", int(filtered_df["operator"].nunique()) if not filtered_df.empty and "operator" in filtered_df else 0)

    # ë¹ˆ ê²°ê³¼ ê°€ì´ë“œ
    if filtered_df.empty:
        st.warning("ì¡°ê±´ì— ë§žëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        with st.expander("ì¶”ì²œ ê²€ìƒ‰ ë³´ê¸°"):
            st.write("â€¢ ì¸ê¸° FEATURE:", ", ".join(distinct_values("feature_name")[:5]))
            st.write("â€¢ ì¸ê¸° ì‚¬ì—…ìž:", ", ".join(distinct_values("operator")[:5]))
            st.write("â€¢ ì¸ê¸° MCC:", ", ".join(distinct_values("mcc")[:5]))
        return filter_state

    # í‘œ ê°€ë…ì„±: status ë°°ì§€í™”
    if "status" in filtered_df.columns:
        def _badge(x: str) -> str:
            x = (x or "").lower()
            if "avail" in x: return "ðŸŸ¢ Available"
            if "pilot" in x: return "ðŸŸ¡ Pilot"
            if "plan" in x or "progress" in x: return "ðŸ”µ Planned"
            return x or "-"
        filtered_df["status"] = filtered_df["status"].map(_badge)

    # í‘œì‹œ ì»¬ëŸ¼ ìš°ì„ ìˆœìœ„
    preferred = ["feature_group","feature_name","model","region","country","operator","mcc","mnc","status","last_updated"]
    cols = [c for c in preferred if c in filtered_df.columns] + [c for c in filtered_df.columns if c not in preferred]

    st.subheader("ê²€ìƒ‰ ê²°ê³¼")
    st.dataframe(filtered_df[cols], hide_index=True, use_container_width=True)

    # CSV ë‹¤ìš´ë¡œë“œ ê°€ë“œ(2ë§Œ í–‰)
    st.divider()
    st.subheader("â¬‡ï¸ CSV ë‹¤ìš´ë¡œë“œ")
    row_limit = 20000
    if len(filtered_df) > row_limit:
        st.error(f"ë³´ì•ˆ ì •ì±…ì— ë”°ë¼ ìµœëŒ€ {row_limit:,}í–‰ê¹Œì§€ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. í˜„ìž¬: {len(filtered_df):,}í–‰")
    else:
        agree = st.checkbox("ì‚¬ë‚´ ë°ì´í„° ë³´ì•ˆ ì •ì±…ì— ë™ì˜í•©ë‹ˆë‹¤.")
        st.caption("â€» ì™¸ë¶€ ë°˜ì¶œ ê¸ˆì§€, ë‚´ë¶€ ì—…ë¬´ ëª©ì ì— í•œí•¨")
        if agree:
            st.download_button(
                "CSV ì €ìž¥",
                data=filtered_df[cols].to_csv(index=False).encode("utf-8-sig"),
                file_name="feature_records.csv",
                mime="text/csv",
            )

    return filter_state
EOF_HOME

echo "==> app/views/visualization.py ìž‘ì„±"
cat > app/views/visualization.py <<'EOF_VIZ'
from __future__ import annotations
import pandas as pd
import streamlit as st
from app.data.sample_features import visualization_capabilities, visualization_group_titles
from app.views.state import FilterState

def render_visualization(filter_state: FilterState, dataframe: pd.DataFrame):
    st.title("ðŸ“ˆ ì‹œê°í™” ìž‘ì—… ê³µê°„")
    st.info("ì´ ì˜ì—­ì€ **ìž‘ì—… ì¤‘(WIP)** ìž…ë‹ˆë‹¤. ì°¨íŠ¸/ì§€ë„ í…œí”Œë¦¿ì€ ë‹¨ê³„ì ìœ¼ë¡œ ì—°ê²°ë©ë‹ˆë‹¤.")
    st.caption("ì„ íƒí•œ FEATURE GROUPì— ë”°ë¼ ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œê°í™”ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤.")

    groups = filter_state.active_visualization_groups(dataframe)
    titles = visualization_group_titles()
    capabilities = visualization_capabilities()

    if not groups:
        st.info("ì‚¬ì´ë“œë°”ì™€ í™ˆ í™”ë©´ì—ì„œ FEATURE GROUPì„ ì„ íƒí•˜ë©´ ì—¬ê¸°ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ì‹œê°í™” ìœ í˜•ì´ ë…¸ì¶œë©ë‹ˆë‹¤.")
        return filter_state

    readable = [titles.get(group, group) for group in groups]
    selected_label = st.selectbox("ì‹œê°í™” ê·¸ë£¹", readable, key="viz_group_selector")
    inverse_titles = {v: k for k, v in titles.items()}
    selected_group = inverse_titles.get(selected_label, selected_label)

    st.markdown("### ì§€ì›ë˜ëŠ” ì‹œê°í™”")
    for item in capabilities.get(selected_group, []):
        st.markdown(f"- {item}")

    # ê°„ë‹¨ ë§‰ëŒ€ê·¸ëž˜í”„(ì°¸ê³ ìš©)
    st.divider()
    st.markdown("### ë¯¸ë¦¬ë³´ê¸°(ìƒ˜í”Œ)")
    base = filter_state.apply(dataframe)
    if base.empty:
        st.info("ì„ íƒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í™ˆì—ì„œ í•„í„°ë¥¼ í™•ìž¥í•´ ë³´ì„¸ìš”.")
        return filter_state

    c1, c2 = st.columns(2)
    with c1:
        st.caption("ëª¨ë¸ë³„ FEATURE ê°œìˆ˜")
        g = base.groupby("model")["feature_name"].nunique().reset_index(name="count").sort_values("count", ascending=False)
        st.bar_chart(g, x="model", y="count", use_container_width=True)
    with c2:
        st.caption("ì‚¬ì—…ìžë³„ FEATURE ê°œìˆ˜")
        g2 = base.groupby("operator")["feature_name"].nunique().reset_index(name="count").sort_values("count", ascending=False)
        g2["operator"] = g2["operator"].fillna("(N/A)")
        st.bar_chart(g2, x="operator", y="count", use_container_width=True)

    return filter_state
EOF_VIZ

echo "==> app/data/sample_features.py ìž‘ì„±"
cat > app/data/sample_features.py <<'EOF_DATA'
from __future__ import annotations
from dataclasses import dataclass
from functools import lru_cache
from typing import Dict, List
import pandas as pd

# Schema (DataFrame columns):
#   feature_group:str, feature_name:str, model:str, mcc:str, mnc:str,
#   region:str, country:str, operator:str, status:str, last_updated:str,
#   visualization_group:str

@dataclass(frozen=True)
class FeatureRecord:
    feature_group: str
    feature_name: str
    model: str
    mcc: str
    mnc: str
    region: str
    country: str
    operator: str
    status: str
    last_updated: str
    visualization_group: str

_DATA: List[FeatureRecord] = [
    FeatureRecord("Roaming Essentials", "Automatic APN Switch", "XR-550", "450", "05", "APAC", "Korea", "SKT", "Available", "2025-05-12", "Full Suite"),
    FeatureRecord("Roaming Essentials", "Dual SIM Failover",  "XR-520", "440", "10", "APAC", "Japan", "DoCoMo", "Pilot",    "2025-04-29", "Full Suite"),
    FeatureRecord("Security Core",     "IMS Firewall",          "XR-550", "208", "01", "EMEA", "France", "Orange", "Available", "2025-02-14", "Tabular Only"),
    FeatureRecord("Security Core",     "Signalling Anomaly Detection", "XR-540", "724", "06", "LATAM", "Brazil", "Vivo", "In Progress", "2025-03-03", "Tabular Only"),
    FeatureRecord("Premium Insights",  "Traffic Heatmap",       "XR-600", "310", "260","NA", "United States", "T-Mobile", "Available", "2025-06-01", "Full Suite"),
    FeatureRecord("Premium Insights",  "KPI Correlation",       "XR-600", "234", "15", "EMEA","United Kingdom","Vodafone", "Planned",   "2025-05-05", "Full Suite"),
    FeatureRecord("Connectivity Essentials", "VoLTE Optimiser", "XR-520", "404", "45", "APAC","India", "Jio", "Pilot", "2025-01-25", "Tabular Only"),
    FeatureRecord("Connectivity Essentials", "NR Fallback Enhancer", "XR-530", "262", "02", "EMEA","Germany","Telekom", "Available", "2025-04-17", "Full Suite"),
]

# ê·¸ë£¹ë³„ ì‚¬ìš© ì°¨ì› / Value íƒ€ìž… / ëª©ë¡ ëª¨ë“œ ì§€ì› (UX ì œì–´ìš©)
FEATURE_GROUP_SCHEMA: Dict[str, Dict] = {
    "Roaming Essentials": {"dims":["region","country","operator","mcc_mnc"], "value_type":"bool",   "list_modes":["allow","block"]},
    "Security Core":      {"dims":["region","country","operator"],            "value_type":"str",    "list_modes":["allow"]},
    "Premium Insights":   {"dims":["region","country"],                         "value_type":"number", "list_modes":["allow"]},
}

@lru_cache(maxsize=1)
def load_feature_dataframe() -> pd.DataFrame:
    frame = pd.DataFrame([r.__dict__ for r in _DATA])
    frame.sort_values(["feature_group", "feature_name"], inplace=True)
    frame.reset_index(drop=True, inplace=True)
    return frame


def distinct_values(column: str) -> List[str]:
    df = load_feature_dataframe()
    return sorted(df[column].dropna().unique().tolist())

@lru_cache(maxsize=None)
def visualization_capabilities() -> Dict[str, List[str]]:
    return {
        "Full Suite": [
            "Feature summary table",
            "Time series / KPI charts",
            "Traffic density heatmap",
            "Geo heatmap (Leaflet)",
        ],
        "Tabular Only": [
            "Feature summary table",
        ],
    }

@lru_cache(maxsize=None)
def visualization_group_titles() -> Dict[str, str]:
    return {
        "Full Suite": "ê·¸ë£¹ 1 â€“ ì „ì²´ ì‹œê°í™” ì§€ì›",
        "Tabular Only": "ê·¸ë£¹ 2 â€“ í‘œ ê¸°ë°˜ ì‹œê°í™”",
    }
EOF_DATA

echo "==> app/data/data_manager.py ìž‘ì„±"
cat > app/data/data_manager.py <<'EOF_DM'
from __future__ import annotations
import os, json, time, pathlib, threading
from datetime import datetime, timedelta, timezone
from typing import Optional, Dict
import pandas as pd
import requests

KST = timezone(timedelta(hours=9))

class DataManager:
    """DRFì—ì„œ í•˜ë£¨ 1íšŒ ë™ê¸°í™”â†’ë¡œì»¬ ìºì‹œ(Parquet)ë¥¼ ì œê³µí•˜ê³ , ì•±ì€ ìºì‹œë§Œ ì¡°íšŒ."""
    def __init__(self, api_base: str, data_dir: str = "./_cache", verify_ssl: bool = False, timeout: int = 20):
        self.api_base = api_base.rstrip("/")
        self.data_dir = pathlib.Path(data_dir)
        self.verify_ssl = verify_ssl
        self.timeout = timeout
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.meta_path = self.data_dir / "_meta.json"
        self.lock = threading.Lock()
        self._meta = self._load_meta()

    def _load_meta(self) -> Dict:
        if self.meta_path.exists():
            try:
                return json.loads(self.meta_path.read_text(encoding="utf-8"))
            except Exception:
                return {}
        return {}

    def _save_meta(self):
        tmp = json.dumps(self._meta, ensure_ascii=False, indent=2)
        self.meta_path.write_text(tmp, encoding="utf-8")

    def _dataset_paths(self, name: str):
        return (self.data_dir / f"{name}.parquet",)

    def last_sync_at(self, name: str) -> Optional[datetime]:
        ts = self._meta.get(name, {}).get("last_sync_epoch")
        if not ts: return None
        return datetime.fromtimestamp(int(ts), tz=KST)

    def _set_sync_meta(self, name: str, etag: Optional[str] = None):
        entry = self._meta.get(name, {})
        entry["last_sync_epoch"] = int(time.time())
        if etag: entry["etag"] = etag
        self._meta[name] = entry
        self._save_meta()

    # -------- DRF í˜¸ì¶œ/ì €ìž¥ -------- #
    def _drf_get(self, path: str, params: Optional[dict] = None, etag: Optional[str] = None):
        url = f"{self.api_base}/{path.lstrip('/') }"
        headers = {}
        if etag:
            headers["If-None-Match"] = etag
        r = requests.get(url, params=params or {}, headers=headers, timeout=self.timeout, verify=self.verify_ssl)
        if r.status_code == 304:
            return None, etag
        r.raise_for_status()
        return r.json(), r.headers.get("ETag")

    def _save_parquet(self, name: str, data):
        df = pd.DataFrame(data["results"] if isinstance(data, dict) and "results" in data else data)
        path, = self._dataset_paths(name)
        df.to_parquet(path, index=False)
        return df

    def sync_feature(self, name="feature1", path="/api/feature1/", params=None) -> pd.DataFrame:
        with self.lock:
            meta = self._meta.get(name, {})
            etag = meta.get("etag")
            res, new_etag = self._drf_get(path, params=params, etag=etag)
            if res is None:
                return self.load(name, stale_ok=True)
            df = self._save_parquet(name, res)
            self._set_sync_meta(name, etag=new_etag or etag)
            return df

    # -------- ë¡œì»¬ ë¡œë“œ/ê°±ì‹  -------- #
    def load(self, name: str, stale_ok=True) -> pd.DataFrame:
        path, = self._dataset_paths(name)
        if not path.exists():
            try:
                return self.sync_feature(name=name)
            except Exception:
                return pd.DataFrame()
        return pd.read_parquet(path)

    def refresh_if_stale(self, name: str, max_age_hours=24) -> None:
        last = self.last_sync_at(name)
        if last and (datetime.now(tz=KST) - last) < timedelta(hours=max_age_hours):
            return
        def _bg():
            try:
                self.sync_feature(name=name)
            except Exception:
                pass
        threading.Thread(target=_bg, daemon=True).start()

    @staticmethod
    def format_last_sync(dt: Optional[datetime]) -> str:
        return dt.strftime("%Y-%m-%d %H:%M:%S KST") if dt else "-"
EOF_DM

echo "==> scripts/sync_data.py ìž‘ì„±"
cat > scripts/sync_data.py <<'EOF_SYNC'
#!/usr/bin/env python3
import os
from app.data.data_manager import DataManager

API_BASE = os.getenv("API_BASE", "https://10.x.x.x")
DATA_DIR = os.getenv("DATA_DIR", "/srv/fmw/_cache")
VERIFY_SSL = os.getenv("VERIFY_SSL", "false").lower() in ("1","true","yes")

if __name__ == "__main__":
    dm = DataManager(api_base=API_BASE, data_dir=DATA_DIR, verify_ssl=VERIFY_SSL)
    df = dm.sync_feature(name="feature1", path="/api/feature1/", params={"limit": 500000})
    print(f"synced rows={len(df)} last_sync={dm.last_sync_at('feature1')}")
EOF_SYNC
chmod +x scripts/sync_data.py

echo "==> íŒ¨í‚¤ì§€ ì¸ì‹ìš© __init__.py ìƒì„±"
for d in app app/core app/views app/data scripts; do
  [ -f "$d/__init__.py" ] || echo > "$d/__init__.py"
done

echo "==> PR í…œí”Œë¦¿/CI/ignore ìž‘ì„±"
mkdir -p .github
cat > .github/pull_request_template.md <<'EOF_PR'
# ECO: Streamlit UX ê°œì„  ì ìš©

## ë³€ê²½ ìš”ì•½
- URL ë”¥ë§í¬(ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°) ë°˜ì˜
- ê·¸ë£¹ ì¸ì§€í˜• ìƒì„¸ í•„í„°(ì°¨ì› í† ê¸€)
- ë¹ˆ ê²°ê³¼ ê°€ì´ë“œ ë° KPI ì¹´ë“œ
- CSV ë‹¤ìš´ë¡œë“œ ê°€ë“œ(ë³´ì•ˆ ë™ì˜ + í–‰ ì œí•œ)
- (On-prem) DRFâ†’Parquet ì¼ì¼ ë™ê¸°í™” / ìºì‹œ ë¡œë”

## ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ë¡œì»¬ êµ¬ë™ í™•ì¸ (`streamlit run main.py`)
- [ ] DRF API ì ‘ê·¼ì„± í™•ì¸ (ì‚¬ë‚´ë§/Cloud ê°ê°)
- [ ] ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì»¬ëŸ¼ í™•ì¸
- [ ] Nginx ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ ê²½ë¡œ(`/fmw`) ì ê²€
- [ ] ë³´ì•ˆ ê°€ì´ë“œ ë³µê¸°(ë‚´ë¶€ ë°˜ì¶œ ê¸ˆì§€)
EOF_PR

cat > .github/workflows/ci.yml <<'EOF_CI'
name: CI
on:
  pull_request:
    branches: [ main, develop ]
jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install minimal deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Import smoke
        run: |
          python - << 'PY'
          import importlib
          for m in ["streamlit","pandas","pyarrow","requests"]:
            importlib.import_module(m)
          print("OK")
          PY
EOF_CI

cat > .gitignore <<'EOF_IGN'
_cache/
**/__pycache__/
*.pyc
.DS_Store
.streamlit/secrets.toml
EOF_IGN

